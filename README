Karmalutional Network
By Rafi Khaled, Colburn Morrison, Nanak Nihal Khalsa, and Jo Hsuan Lee

This is a neural network that predicts reddit comment popularity. It outputs a value from 1-10 with 50% test set accuracy. An output of  1 represents a comment below the 10th percentile of scores. By the same token, a 2 represents a comment that is >= 10th and < the 20th percentiles; a 10 represents a comment >= the 90th percentile. 


Unlike the name implies, this model does not use a convolutional network. However, it is arguably catchy so it has been kept. Instead, it uses a recurrent neural network. Recurrent neural networks are different than feedforward neural networks because they involve feeding their outputs as inputs to themselves. However, in many ways, they are similar to feedforward neural networks. 

Feedforward neural networks consist of non-linear transformations of an input matrix into an output matrix. They can be thought of as having layers. The first layer in a neural network is called the input layer, the last is called the output layer, and anything between is a hidden layer. Each layer passes
its output to the next layer. A layer can be thought of as an arrangement of nodes, often visualized in columns (see NN.png for a diagram). Each node represents a weight value and a bias value. The inputs to any node are added together and multiplied by the weight before a bias is added to them. The resulting value is passed into a nonlinear function, such as the sigmoid or ReLU function, and the result is considered the output of the node, and the input for the next.

Non-linearity is important because it allows the network to process highly complex neuron networks. It is a mystery how neural networks can approximate outputs through a series of transformation functions with such high accuracy, but it is certain that they would only be able to approximate simple networks without nonlinear activation functions between the layers. 

Computationally, the nodes are not treated as individual entities. The concept of nodes is primarily used for human understanding. Computationally, layers of m nodes are treated as two matrices: one (1 x m) column vector of biases and another (n x m) matrix, where n is the number of outputs from the previous layer.
The output (1 x n) matrix from the previous layer is multiplied by (m x n) weights to result in a (1 x m) matrix, to which the (1 x m) biases are added. To each value in the resulting (1 x m) matrix, a non-linear activation function is applied and passed in again as the input to the next hidden layer if there is one. Otherwise, it is the output of the network. This is a common architecture for a feedforward network, yet there are exceptions. 

The above is a standard feedforward neural network computation, widely known as Forward Propagation. However, in order to train a more accurate network, we need Backward Propagation, or Back Prop, to achieve the appropriate weights and biases.

Back Prop is possible when the desired output (i.e. 1 or 0, cat or non-cat) for any input (i.e. an image that may or may not contain a cat) is known. On the flip side, Back Prop is not possible if the activation functions are non-differentiable, hence networks rarely use non-differentiable activation functions. 

One step in Back Prop is comparing the output of the network for a given input to the desired output for that input. It uses the "loss function" to determine a number that represents how close the output matches the desired output, and our goal is to minimize this loss function. The network does this by computing the derivative of the function with respect to the weights and another with respect to the biases. It takes the derivative at the current value and uses this to determine how much to change the current weight or bias in question by. This is done by 
w = w - learning_rate * dJ(w)/dw and 
b = b - learning_rate * dJ(b)/db, 
where w is a weight and b is a bias. With a greater learning_rate, the network will learn faster but may skip over the minimum by subtracting too much. 

In a nutshell, this is how neural networks work. However, there are more specifics and variations of this design that have not been covered.

RNNs are based on the idea that previous outputs should be incorporated in the current input. An example of where this is helpful is natural language processing. Consider somebody saying "The beach has red sand, which is a result of high iron concentration." Now consider somebody saying "That was a
delicious sandwich." The network should not transcribe sand, which as sandwich, nor should it transcribe sandwich as sand, which. It needs previous words to better determine the meaning of the current word. Reddit comments are similar; the meaning of a current word also depends on the meaning of previous words. Recurrent neural networks have been shown to be effective at
NLP tasks such as understanding text. 

The specific type of RNN used for this project is a LSTM. LSTM, or Long Short-Term memory, nodes are a type of node in RNNs that allow the network to 'remember' longer-term dependencies in sentences. LSTMs work, in a broad sense, by computing for each node the probability that node should update its value based on the previous nodes' output. This update does not happen every time, so various characteristics are "remembered" throughout the network, for example noun-verb agreements. Furthermore, since values aren't changing as often, the possibly for vanishing gradients decreases.


