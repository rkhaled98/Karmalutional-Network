# -*- coding: utf-8 -*-
"""word_embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YVuQRV09QYao0-f5Oxu6x-3O9QdmVony
"""

import numpy as np
from tensorflow import keras
from google.colab import files


#note: this causes the environment to crash and restart with such a huge file. for now, we are just using wget instead of Google Drive.
#wget is surprisingly fast for this task, possibly moreso than Google Drive would be.

#from documentation at https://colab.research.google.com/notebook#fileId=/v2/external/notebooks/io.ipynb&scrollTo=vz-jH8T_Uk2c:
#authenticate to access Google Drive using the REST API
#from google.colab import auth
#auth.authenticate_user()

#create a drive client:
#from googleapiclient.discovery import build
#drive_service = build('drive', 'v3')

#getting the glove file

#import io
#from googleapiclient.http import MediaIoBaseDownload

#file_id = '18DZMqkgq5_nXgDEk9zc1HT7twmLuMEBk'
#request = drive_service.files().get_media(fileId = file_id)
#glove_downloaded = io.BytesIO()
#downloader = MediaIoBaseDownload(glove_downloaded, request)

#done = False
#while done is False:
#  _, done = downloader.next_chunk()
#glove_downloaded.seek(0)



#download the glove word embedding from Stanford NLP:
!wget http://nlp.stanford.edu/data/glove.42B.300d.zip

!unzip glove.42B.300d.zip

#files = files.upload()
#Inspired by https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py
#and the Emojify exercise from Andrew Ng's course on sequence models
word_to_index = {}
word_to_vector = {} #not to be confused with word2vec

#open the file:
with open('glove.42B.300d.txt') as f:
  i = 0
  for line in f:
    split_line = line.split()
    word = split_line[0]
    weights = np.asarray(split_line[1:])
    #store the weight vector at the appropriate index:
    word_to_vector[word] = weights
    word_to_index[word] = i
    i += 1 #no ++ in python



#generates a Keras embedding layer, inspired by Emojify in Andrew Ng's Sequence Models Coursera course
def gen_embedding_layer():
  input_size = len(word_to_index) + 1 #Keras requires this to be the vocab size + 1
  output_size = word_to_vector["bamboozled"].shape[0]
  
  embedding_matrix = np.zeros((input_size, output_size))
  for word, index in word_to_index.items():
    embedding_matrix[index, :] = word_to_vector[word] 
 

  layer = keras.layers.Embedding(input_size, output_size, trainable=False)
  layer.build((None,))
  layer.set_weights([embedding_matrix])
  return layer
  
gen_embedding_layer()

#Creating a new embedding layer:
def create_embedding_layer(file='word_embeddings.txt'):
  embedding_dataframe = pd.read_csv(file)
  word-to-vec
