import csv
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# from sklearn.metrics import confusion_matrix

import numpy as np
from model_utils import *
# import matplotlib.pyplot as plt

# %matplotlib inline

print("reading training data")
X_train, Y_train = read_csv('data/news_news_small_1.csv') # the data needed by the read_csv is a csv with comment,rank generated by the datasets main script 
print("done reading training data")
filtered = list(filter(None, Y_train))
Y_train = np.asarray(filtered)
Y_train = Y_train.astype(int)
#print(Y_train) make sure theyre ints
print("done reading testing data")
X_test, Y_test = read_csv('data/news_news_small_test_1.csv')
print("done reading testing data")
filtered = list(filter(None, Y_test))
Y_test = np.asarray(filtered)
Y_test = Y_test.astype(int)
#print(Y_test) make sure theyre ints

# Y_train = np.append(Y_train, [0])
print(X_train.shape, Y_train.shape)
# Y_test = np.append(Y_test, [0])
print(X_test.shape, Y_test.shape)

def read_glove_vecs(glove_file):
    with open(glove_file, 'r') as f:
        words = set()
        word_to_vec_map = {}
        for line in f:
            line = line.strip().split()
            curr_word = line[0]
            words.add(curr_word)
            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)
        
        i = 1
        words_to_index = {}
        index_to_words = {}
        for w in sorted(words):
            words_to_index[w] = i
            index_to_words[i] = w
            i = i + 1
    return words_to_index, index_to_words, word_to_vec_map

print("reading glove vector")
word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')
print("done reading glove vector")

def remove_non_keys(X, dict):
    for i in range(0, len(X)):
        for word in X[i].split(" "):
            if (not (word in dict)):
                if (not (word == " ")):
                    if (not (word == "")):
                        X[i] = X[i].replace(word, '')
                        X[i] = " ".join(X[i].split())
    return X

print("removing non keys")
X_train = remove_non_keys(X_train, word_to_index)
X_train = remove_non_keys(X_train, word_to_index)
X_test = remove_non_keys(X_test, word_to_index)
X_test = remove_non_keys(X_test, word_to_index)
print("done removing non keys")

def sentence_to_avg(sentence, word_to_vec_map):
    """
    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word
    and averages its value into a single vector encoding the meaning of the sentence.
    
    Arguments:
    sentence -- string, one training example from X
    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation
    
    Returns:
    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)
    """
    # Split sentence into list of lower case words (around 1 line)
    words = [i.lower() for i in sentence.split()]

    # Initialize the average word vector, should have the same shape as your word vectors.
    avg = np.zeros((50,))
    
    #average the word vectors. You can loop over the words in the list "words".
    for w in words:
        avg += word_to_vec_map[w]
    avg = avg / len(words)

    
    return avg

def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):
    """
    Model to train word vector representations in numpy.
    
    Arguments:
    X -- input data, numpy array of sentences as strings, of shape (m, 1)
    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)
    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation
    learning_rate -- learning_rate for the stochastic gradient descent algorithm
    num_iterations -- number of iterations
    
    Returns:
    pred -- vector of predictions, numpy-array of shape (m, 1)
    W -- weight matrix of the softmax layer, of shape (n_y, n_h)
    b -- bias of the softmax layer, of shape (n_y,)
    """
    
    np.random.seed(1)

    # Define number of training examples
    m = Y.shape[0]                          # number of training examples
    n_y = 10                                 # number of classes  
    n_h = 50                                # dimensions of the GloVe vectors 
    
    # Initialize parameters using Xavier initialization
    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)
    b = np.zeros((n_y,))
    
    # Convert Y to Y_onehot with n_y classes
    Y_oh = convert_to_one_hot(Y, C = n_y) 
    
    # Optimization loop
    for t in range(num_iterations):                       # Loop over the number of iterations
        for i in range(m):                                # Loop over the training examples
            
            # Average the word vectors of the words from the i'th training example
            avg = sentence_to_avg(X[i], word_to_vec_map)

            # Forward propagate the avg through the softmax layer
            z = np.dot(W, avg) + b
            a = softmax(z)

            # Compute cost using the i'th training label's one hot representation and "A" (the output of the softmax)
            cost = -np.sum(np.multiply(Y_oh[i], np.log(a)))
            
            # Compute gradients 
            dz = a - Y_oh[i]
            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))
            db = dz

            # Update parameters with Stochastic Gradient Descent
            W = W - learning_rate * dW
            b = b - learning_rate * db
        
        if t % 100 == 0:
            print("Epoch: " + str(t) + " --- cost = " + str(cost))
            pred = predict(X, Y, W, b, word_to_vec_map)

    return pred, W, b

pred, W, b = model(X_train, Y_train, word_to_vec_map)
print(pred)

print("Training set:")
pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)
print('Test set:')
pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)

X_sample = np.array(['the entire article talks about his race reddit you are a disappointing piece of shit', 'so more sponsored posts too bad voatco is such a shitty alternative', 'so is his job to come around and censor our posts'])
Y_sample = np.array([7, 8, 6])
X_sample = remove_non_keys(X_sample, word_to_index)
X_sample = remove_non_keys(X_sample, word_to_index)
Y_sample = remove_non_keys(Y_sample, word_to_index)
Y_sample = remove_non_keys(Y_sample, word_to_index)
pred_sample = predict(X_sample, Y_sample, W, b, word_to_vec_map, output_prediction=True)