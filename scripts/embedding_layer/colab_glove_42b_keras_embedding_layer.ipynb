{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1YVuQRV09QYao0-f5Oxu6x-3O9QdmVony",
          "timestamp": 1524447433189
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-Jitrv8mIngM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QnalTZlN0k0c",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#note: this causes the environment to crash and restart with such a huge file. for now, we are just using wget instead of Google Drive.\n",
        "#wget is surprisingly fast for this task, possibly moreso than Google Drive would be.\n",
        "\n",
        "#from documentation at https://colab.research.google.com/notebook#fileId=/v2/external/notebooks/io.ipynb&scrollTo=vz-jH8T_Uk2c:\n",
        "#authenticate to access Google Drive using the REST API\n",
        "#from google.colab import auth\n",
        "#auth.authenticate_user()\n",
        "\n",
        "#create a drive client:\n",
        "#from googleapiclient.discovery import build\n",
        "#drive_service = build('drive', 'v3')\n",
        "\n",
        "#getting the glove file\n",
        "\n",
        "#import io\n",
        "#from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "#file_id = '18DZMqkgq5_nXgDEk9zc1HT7twmLuMEBk'\n",
        "#request = drive_service.files().get_media(fileId = file_id)\n",
        "#glove_downloaded = io.BytesIO()\n",
        "#downloader = MediaIoBaseDownload(glove_downloaded, request)\n",
        "\n",
        "#done = False\n",
        "#while done is False:\n",
        "#  _, done = downloader.next_chunk()\n",
        "#glove_downloaded.seek(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iNNgBGagDV5L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#download the glove word embedding from Stanford NLP:\n",
        "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iKvrF3OKDk11",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "50f935d9-6d11-4954-a580-24e08a87d072",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524295053853,
          "user_tz": 240,
          "elapsed": 53545,
          "user": {
            "displayName": "Nanak Nihal Khalsa",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100787844473172298543"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip glove.42B.300d.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.42B.300d.zip\n",
            "  inflating: glove.42B.300d.txt      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vb9h6EuR3xJ6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#files = files.upload()\n",
        "#Inspired by https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
        "#and the Emojify exercise from Andrew Ng's course on sequence models\n",
        "word_to_index = {}\n",
        "word_to_vector = {} #not to be confused with word2vec\n",
        "\n",
        "#open the file:\n",
        "with open('glove.42B.300d.txt') as f:\n",
        "  i = 0\n",
        "  for line in f:\n",
        "    split_line = line.split()\n",
        "    word = split_line[0]\n",
        "    weights = np.asarray(split_line[1:])\n",
        "    #store the weight vector at the appropriate index:\n",
        "    word_to_vector[word] = weights\n",
        "    word_to_index[word] = i\n",
        "    i += 1 #no ++ in python\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s6y82v_vICom",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TSrqt-fF7UwA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#generates a Keras embedding layer, inspired by Emojify in Andrew Ng's Sequence Models Coursera course\n",
        "def gen_embedding_layer():\n",
        "  input_size = len(word_to_index) + 1 #Keras requires this to be the vocab size + 1\n",
        "  output_size = word_to_vector[\"bamboozled\"].shape[0]\n",
        "  \n",
        "  embedding_matrix = np.zeros((input_size, output_size))\n",
        "  for word, index in word_to_index.items():\n",
        "    embedding_matrix[index, :] = word_to_vector[word] \n",
        " \n",
        "\n",
        "  layer = keras.layers.Embedding(input_size, output_size, trainable=False)\n",
        "  layer.build((None,))\n",
        "  layer.set_weights([embedding_matrix])\n",
        "  return layer\n",
        "  \n",
        "gen_embedding_layer()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}